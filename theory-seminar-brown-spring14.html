
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>


<meta content="text/html; charset=UTF-8" http-equiv="content-type">
<title>Brown ICERM Theory Seminar</title>

<link href="css/bootstrap.css" rel="stylesheet">
<link href="starter-template.css" rel="stylesheet"> 
<link rel="stylesheet" href="files/font-awesome-4.0.3/css/font-awesome.min.css">

<style type="text/css">
.title {font-weight: bold; }
.abs {
background-color: #eee;
padding: 5px;
display: none;
}
</style>

<script type="text/javascript">
function showAbstract(e){
f = e;
var div;
for(div = e.nextSibling; div.className != "abs"; div = div.nextSibling);

if (div.style.display=="block"){
div.style.display="";
} else {
div.style.display="block";
}
return true;
}
</script>

</head>

<body>
<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
<div class="container">
<div class="navbar-header">
<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
<span class="sr-only">Toggle navigation
<span class="icon-bar">
<span class="icon-bar">
<span class="icon-bar">
</button>
<a class="navbar-brand" href="#">Brown ICERM Theory Seminar</a>
<a class="navbar-brand" href="#organizers">Organizers</a>
<a class="navbar-brand" href="#information">Information</a>
<a class="navbar-brand" href="#schedule">Schedule</a>
<a class="navbar-brand" href="#archives">Archives</a>
<!--<a class="navbar-brand" href="#calendar">Calendar</a>
-->
</div>
</div>
</div>

<div class="container">

<div class="starter-template">
<br/>
<br/>
<br/>

<h1><img src="pics/brown.jpeg" style="width:50pt">  <font color="brown">Brown ICERM Theory Seminar</font>
</h1>

<a id="organizers"><h3>Organizers: 
</h3></a>
<ul>
<li><b><a class="name" href="http://cs.brown.edu/people/tdickerson/">Thomas Dickerson</a></b> <tt>thomas_dickerson (at) brown.edu</tt>
<li> <b><a class="name" href="http://grigory.us">Grigory Yaroslavtsev</a></b> <tt>grigory (at) grigory.us</tt>
</ul>
</h3>


<a id="information"><h3> Information </h3></a>
<ul>

<li> We meet <b>Fridays 1pm &ndash; 2pm at ICERM, 11th Floor Lecture Hall</b>, except for extra sessions and changes highlighted in red below.  
<li> If you are interested in giving a talk, please, contact the organizers.			
<li> Talk announcements will appear on the <a rel="nofollow" target="_blank" href="https://lists.cs.brown.edu/sympa/info/theory">Theory List</a> and announced by Lauren to the program participants. Please, stay tuned! 
</ul>				     



<a id="schedule"> <h3> Schedule </h3></a>
<br>

<ul class="fa-ul">
<p><li> <i class="fa li fa fa-group"> </i>
Friday, May 09, 2014: 

<b> 
<a class="name" href="http://theory.stanford.edu/~tim/">Tim Roughgarden</a></b> (Stanford University, CS)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
TBD
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
</div>

<p>

</li>

<p><li> <i class="fa li fa fa-group"> </i>
Friday, May 02, 2014: 

<b> 
<a class="name" href="http://people.csail.mit.edu/moitra/ ">Ankur Moitra</a></b> (MIT, CSAIL)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
TBD
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
</div>
</li>


<p><li> <i class="fa li fa fa-group"> </i>
Friday, April 25, 2014: 

<b> 
<a class="name" href="">TBD</a></b> (TBD)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
TBD
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
</div>
</li>

<p><li> <i class="fa li fa fa-group"> </i>


Friday, April 18, 2014: 

<b><a class="name" href="http://grigory.us/big-data-day.html" style="text-decoration: underline">Sublinear Algorithms and Big Data Day</a></b>
</li>

<p><li> <i class="fa li fa fa-warning"> </i>

Friday, April 11, 2014: 

<b><font color="brown">Workshop, no seminar.</font></b>

</li>

<p><li> <i class="fa li fa fa-group"> </i>
Friday, April 04, 2014: 

<b> 
<a class="name" href="http://www.cs.cmu.edu/~venkatg/">Venkatesan Guruswami</a></b> (Carnegie-Mellon University and Microsoft Research, New England)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Superlinear Lower Bounds for Multipass Graph Processing
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
The data stream model has emerged as a popular algorithmic paradigm for computation on massive data sets. With large graphs arising naturally in many contexts, graph problems have also been considered in the streaming model. However, streaming algorithms for even simple graph-theoretic tasks require space proportional to the number of vertices. The semi-streaming setting that allows quasi-linear (n poly(log n)) space has therefore been identified as a potential "sweet-spot" for graph streaming.

In this talk, we will rule out multipass semi-streaming algorithms for some basic graph problems. Specifically, we will present n^(1+Omega(1/p)) lower bounds for the space complexity of p-pass streaming algorithms for the following basic problems on n-vertex graphs:

<ul>
<li> testing if a graph has a perfect matching,
<li> computing the distance between two specific vertices in an undirected graph, and
<li> testing if there is a directed path between two vertices in a directed graph.
</ul>

Before our result, it was known that these problems require Omega(n^2) space in one pass, but no n^(1+Omega(1)) lower bound was known for two or more passes.

Our result follows from a communication complexity lower bound for a communication game in which the players hold two graphs on the same set of vertices. The task of the players is to find out whether the sets of vertices reachable from a specific vertex in exactly p+1 steps intersect. We show that this game requires a significant amount of communication if the players are forced to speak in a specific difficult order. Our proof proceeds via an information cost lower bound for a decision version of the classic pointer chasing problem and a direct sum type theorem for the disjunction of several instances of this problem.

Joint work with Krzysztof Onak.
</div>
</li>

<p><li> <i class="fa li fa fa-warning"> </i>
Friday, March 28, 2014: 

<b><font color="brown">Spring Break, no seminar.</font></b> 

</li>


<p><li> <i class="fa li fa fa-group"> </i>
Friday, March 21, 2014, 1:30pm-2:30pm: 

<b> 
<a class="name" href="http://www2.tepper.cmu.edu/andrew/ravi/">R. Ravi</a></b> (Carnegie Mellon University)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Improved Approximations for Graph-TSP in Regular Graphs
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
A tour in a graph is a connected walk that visits every vertex at
least once, and returns to the starting vertex. We give improved
approximation results for a tour with the minimum number of edges
in regular graphs.
<p> 
For cubic bipartite graphs, we provide a polynomial-time
(9/7)-approximation algorithm for minimum tours. For connected
d-regular graph with n vertices, we provide a method that
constructs a tour of length at most (1 + O(d<sup>-1/2</sup>))n, improving
the previous result of Vishnoi (2012) that demonstrated a tour of
length at most (1 + O((log d)<sup>-1/2</sup>))n.
<p>  
The former result uses the cubic bipartite graph structure to find
a cycle cover with large average length. The latter finds a
spanning tree with few odd-degree vertices and augments it to a
tour. Finding such spanning trees to augment is related to the
linear arboricity conjecture of Akiyama, Exoo and Harary (1981), or
alternatively, to a conjecture of Magnant and Martin (2009)
regarding the path cover number of regular graphs.
<p>   
Joint work with Uriel Feige (Weizmann Institute), Jeremy Karp (CMU)
and Mohit Singh (Microsoft Research).
</div>
</li>

<p><li> <i class="fa li fa fa-group"> </i>
Friday, March 14, 2014: 

<b> 
<a class="name" href="http://research.microsoft.com/en-us/people/slivkins/">Alex Slivkins</a></b> (Microsoft Research, NYC)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Bandits with Knapsacks
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
Multi-armed bandit problems are the predominant theoretical model of exploration-exploitation tradeoffs in machine learning, and they have countless applications ranging from medical trials, to communication networks, to Web search and advertising, to dynamic pricing. In many of these application domains the learner may be constrained by one or more supply (or budget) limits, in addition to the customary limitation on the time horizon. The literature lacks a general model encompassing these sorts of problems. We introduce such a model, called ``bandits with knapsacks'', that combines aspects of stochastic integer programming with online learning. A distinctive feature of our problem, in comparison to the existing regret-minimization literature, is that the optimal policy for a given latent distribution may significantly outperform the policy that plays the optimal fixed arm. Consequently, achieving sublinear regret in the bandits-with-knapsacks problem is significantly more challenging than in conventional bandit problems.
We present two algorithms whose reward is close to the information-theoretic optimum: one is based on a novel ``balanced exploration'' paradigm, while the other is a primal-dual algorithm that uses multiplicative updates. Further, we prove that the regret achieved by both algorithms is optimal up to polylogarithmic factors.
<p> 
Joint work with Robert Kleinberg and Ashwinkumar Badanidiyuru. Appeared at FOCS 2013. 
<p>
Full version can be found <a rel="nofollow" target="_blank" href="http://arxiv.org/abs/1305.2545 ">here</a>.
</div>
<p>
</li>

<p><li> <i class="fa li fa fa-group"> </i>
Monday, March 10, 2014, 3-4pm: 

<b> 
<a class="name" href="http://scholar.google.com/citations?user=i5PazXwAAAAJ">Howard Karloff</a></b> (Yahoo! Research, NYC)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Maximum Entropy Summary Trees</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
Given a very large, node-weighted, rooted tree on, say, n nodes,
if one has only enough space to display a k-node summary of the tree,
what is the most informative way to draw the tree?  We define a type of
weighted tree that we call a "summary tree" of the original tree, that
results from aggregating nodes of the original tree subject to certain
constraints. We suggest that the best choice of which summary tree to
use (among those with a fixed number of nodes) is the one that maximizes
the information-theoretic entropy of a natural probability distribution
associated with the summary tree, and we provide a (pseudopolynomial-time)
dynamic-programming algorithm to compute this maximum entropy summary
tree, when the weights are integral. The result is an automated way
to summarize large trees and retain as much information about them as
possible, while using (and displaying) only a fraction of the original
node set.  We also provide an additive approximation algorithm and
a greedy heuristic that are faster than the optimal algorithm, and
generalize to trees with real-valued weights.
<p>
This is joint work with Ken Shirley of ATT Labs and Richard Cole
of NYU.

</div>
<p>
</li>

<p><li> <i class="fa li fa fa-group"> </i>
Friday, March 07, 2014: 

<b> 
<a class="name" href="http://www.eecs.northwestern.edu/hartline ">Jason Hartline</a></b> (Northwestern University and Harvard University)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
The Simple Economics of Approximately Optimal Auctions
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
The intuition that profit is optimized by maximizing marginal revenue
is a guiding principle in microeconomics.  In the classical auction
theory for agents with linear utility and single-dimensional
preferences, Bulow and Roberts (1989) show that the optimal
auction of Myerson (1981) is in fact optimizing marginal revenue.
In particular Myerson's virtual values are exactly the derivative of
an appropriate revenue curve.
<p>
In this talk I will generalize the theory of marginal revenue maximization
from the ideal setting above, to agents with multi-dimensional and
non-linear utility.  This is a main challenge area for mechanism design.
I will show that marginal revenue maximization, though no longer optimal,
continues to be approximately optimal quite generally.  Moreover, the
result gives a reduction from auction design for multi-dimensional
non-linear agents to auction design for single-dimensional linear agents.
The latter being the most studied setting of auction theory.  This
approximate reduction implies that many results automatically
approximately extend.
<p>
Joint work with Saeed Alaei, Hu Fu, and Nima Haghpanah; appeared
in FOCS 2013.
</div>
</li>


<p><li> <i class="fa li fa fa-group"> </i>
Friday, February 28, 2014: 

<b> 
<a class="name" href="http://web.engr.illinois.edu/~kylefox2/">Kyle Fox</a></b> (Brown University, ICERM)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Optimal Cuts in Surface Embedded Graphs
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
In this talk, I will describe some recent algorithmic results related to computing optimal flows and cuts in surface embedded graphs. These results are motivated by a desire to bring our understanding of efficient planar flow and cut algorithms to more general classes of graphs.
<p>
I will begin by sketching an algorithm to compute a global minimum cut in an embedded genus g undirected graph. The algorithm runs in time g<sup>O(g)</sup> n log log n. For constant g, this running time is a polylogarithmic improvement over the best known running time for sparse graphs due to Karger. Unlike Karger’s algorithm, the one I will describe is deterministic
<p>
Second, I will describe an algorithm for counting minimum s,t-cuts in an embedded genus g directed graph. This problem has applications in image segmentation, and despite the problem being #P-complete in general, the algorithm I will describe runs in 2<sup>O(g)</sup> n<sup>2</sup> time. The algorithm can also be used to aid in sampling minimum s,t-cuts uniformly at random.
<p>
When the genus is constant, both algorithms I will describe match the running time of the best known planar graph algorithms for their respective problems. I will conclude the talk by discussing some future work and open problems in this area of research.
<p>
This talk is based on work done with Erin W. Chambers, Jeff Erickson, and Amir Nayyeri.
</div>
</li>


<p><li> <i class="fa li fa fa-group"> </i>
Friday, February 21, 2014: 

<b> 
<a class="name" href="http://www.ilyaraz.org/">Ilya Razenshteyn</a></b> (MIT, CSAIL)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Beyond Locality-Sensitive Hashing
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
We present a new data structure for the c-approximate near neighbor problem (ANN) in the Euclidean space.
For n points in R<sup>d</sup>, our algorithm achieves O(dn<sup>&rho;</sup>) query time and O(n<sup>1 + &rho;</sup> + nd) space, where &rho; &le; 7/(8c<sup>2</sup>) + O(1/c<sup>3</sup>) + o(1). 
This is the first improvement over the result by Andoni and Indyk (FOCS 2006) and
the first data structure that bypasses a locality-sensitive hashing lower bound
proved by O'Donnell, Wu and Zhou (ICS 2011).
By a standard reduction we obtain a data structure for the Hamming space and &#8467;<sub>1</sub> norm with
&rho; &le; 7/(8c) + O(1/c<sup>3/2</sup>) + o(1), which is the first improvement over the result of Indyk and Motwani (STOC 1998).
<p>
Our data structure is able to bypass the locality-sensitive hashing barrier by using data-dependent hash functions (previous work used functions that are oblivious to the data).
<p>
Joint work with Alexandr Andoni, Piotr Indyk and Nguyễn Lê Huy.
A short summary of the paper can be found at <a rel="nofollow" target="_blank" href="http://mittheory.wordpress.com/2013/11/22/beyond-lsh/">http://mittheory.wordpress.com/2013/11/22/beyond-lsh/</a>.
</div>
</li>

<p><li> <i class="fa li fa fa-group"> </i>
Friday, February 14, 2014: 

<b> 
<a class="name" href="http://people.orie.cornell.edu/shmoys/">David Shmoys</a></b> (Cornell University)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">Improving Christofides' Algorithm for the s-t Path Traveling Salesman Problem</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
In 1976, Christofides gave an approximation algorithm for the traveling salesman problem (TSP) with metric costs that was guaranteed to find a tour that was no more than 3/2 times the length of the shortest tour visiting a given set of n cities; it remains an open problem to give a polynomial-time algorithm with a better performance guarantee. There have been a number of recent results that yield improvements for significant special cases, and for related problems. In this talk, we shall present an approximation algorithm for the s-t path TSP with metric costs, which is guaranteed to find a solution of cost within a factor of the golden ratio of optimal in polynomial time; in this variant, in addition to the pairwise distances among n points, there are two prespecified endpoints s and t, and the problem is to find a shortest Hamiltonian path between s and t. Hoogeveen showed that the natural variant of Christofides' algorithm for this problem is a 5/3-approximation algorithm, and this asymptotically tight bound has been the best approximation ratio known until now. We modify this algorithm so that it chooses the initial spanning tree based on an optimal solution to a natural linear programming relaxation, rather than a minimum spanning tree; we prove this simple but crucial modification leads to an improved approximation ratio, surpassing the 20-year-old barrier set by the natural Christofides' algorithm variant.
<p>
This is joint work with Hyung-Chan An and Robert Kleinberg.

</div>

<p>

</li>


<p><li> <i class="fa li fa fa-group"> </i>
Friday, February 07, 2014: 

<b> 
<a class="name" href="https://sites.google.com/site/silviolattanzi/">Silvio Lattanzi</a></b> (Google Research, NYC)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Large-Scale Graph Mining
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
The amount of data available and requiring analysis has grown at astonishing rate in recent years. To cope with this deluge of information it is fundamental to design new algorithms to analyze data efficiently. In this talk, we describe our effort to build a large scale graph-mining library. We first describe the general framework and few relevant problems that we are trying to solve. Then we describe in details two results on local algorithms for clustering and on learning noisy feedback from a crowdsource system.
<p>
To solve the first problem we develop a random-walked based method with better theoretical guarantee compared to all previous work, both in terms of the clustering accuracy and the conductance of the output set. We also prove that our analysis is tight, and perform empirical evaluation to support our theory on both synthetic and real data.
<p>
For the second problem we introduce a new model of Gaussian mixtures, that captures the setting where the data points correspond to ratings on a set of items provided by users who have widely varying expertise. In this setting we study the single item case and obtain efficient algorithms for the problem, complemented by near-matching lower bounds; we also obtain preliminary results for the multiple items case.
<p>
Joint work with Flavio Chierichetti, Anirban Dasgupta, Ravi Kumar, Vahab Mirrokni and Zeyuan Allen Zhu.
</div>
</li>

<p><li> <i class="fa li fa fa-group"> </i>
Friday, January 31, 2014: 

<b> 
<a class="name" href="http://grigory.us/">Grigory Yaroslavtsev</a></b> (Brown University, ICERM)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Approximating Large Graph Problems: The Old and the New
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
In this talk I will discuss new approximation algorithms for multiple classes of classical problems in large graphs. Such algorithms can be used to remove redundancy in distributed systems, cut their costs, simplify the structure of a large network, cluster a set of data points, etc.
<p>
For general directed graphs I will focus on sparsification with distance and connectivity preserving guarantees (directed spanners and Steiner forests). For planar graphs I will discuss algorithms for a class of problems, which have costs associated with nodes of the graph. 
<p>
I will also discuss new algorithms for modern massive parallel computational models. I will show new sketching techniques for geometric graphs in Euclidean space, which allow to compute approximate minimum spanning tree and minimum cost bichromatic matching for huge graphs with a small amount of communication overhead.
<p>
This talk is based on multiple papers by the speaker in collaboration with Alexandr Andoni, Piotr Berman, Arnab Bhattacharyya, Krzysztof Onak, Aleksandar Nikolov, Konstantin Makarychev and  Sofya Raskhodnikova.
</div>
</li>


<p><li> <i class="fa li fa fa-group"> </i>
Friday, January 24, 2014, 

<b> 
<a class="name" href="http://www.nruozzi.me/">Nicholas Ruozzi</a></b> (Columbia University, Institute for Data Sciences and Engineering)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Dynamic Programming, Lifts of Graphs, and Counting Problems
</a>
<!--
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
-->
<div class="abs">
<p>
Recent advances in the study of approximate inference algorithms in the machine learning community have led to provable lower bounds for certain families of counting problems.  I'll explain how simple dynamic programming algorithms combined with "lifts" of graphs have led to these new lower bounds, and I'll discuss a number of interesting conjectures related to classic counting problems in computer science.  No prior knowledge of graphical models and/or machine learning is required.
</div>
</li>

<!--
<li>
Friday, December 13, 2013: 

<b> 
<a class="name" href="http://people.csail.mit.edu/mirrokni/Welcome.html">
Vahab Mirrokni</a></b> (Google Research, NYC) ,
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">Online Stochastic Ad Allocation: Simultaneous and Bicriteria Approximations
</a>
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/vahab.pdf"><font color="green"><b>Slides</b></font></a>]
<div class="abs">
<p>
As an important component of any ad serving system, online capacity (or budget) planning is a central problem in online ad allocation. I will survey primal-based and dual-based techniques borrowed from online stochastic matching literature and report theoretical approximation guarantees and practical evaluations of these algorithms on Google display ads systems serving billions of ads. Then, inspired by practical applications, I will discuss a Simultaneous approximation results for both adversarial and stochastic models and present some recent theoretical results in this context. Finally, I will describe a multi-objective online approximation algorithm for maximizing weight and cardinality of the matching at the same time. I will conclude with several open problems.

<p>

Vahab Mirrokni is a Senior Staff Research Scientist and the manger of the algorithms research group at Google Research, New York. He joined Google after two years at Microsoft Research, and a year at MIT and Amazon. He received his PhD from MIT in 2005 and his B.Sc. from Sharif University in 2001. Vahab is a co-winner of a SODA best student paper award and an ACM EC best paper award.  Vahab's research interests  include algorithmic game theory, large-scale and social network analysis, and approximation algorithms. At Google, he is mainly working on algorithmic and economic problems related to search and online advertising. Recently he is working on online ad allocation problems, distributed algorithms for large-scale graph mining, and mechanism design for advertising exchanges. 
</div>
</li>


<li>
Friday, December 06, 2013: 

<b>
<a class="name" href="http://www.cs.columbia.edu/~rocco/">
Rocco Servedio</a></b> (Columbia University, CS) , 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Learning from Satisfying Assignments</a>
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/rocco.pptx"><font color="green"><b>Slides</b></font></a>]
<div class="abs">
<p>
We introduce and study a new type of learning problem for probability distributions over the Boolean hypercube.  As in the standard PAC learning model, a learning problem in our framework is defined by a class C of Boolean functions over the hypercube, but unlike the standard PAC model, in our model the learning algorithm is given uniform random satisfying assignments of an unknown function in C and its goal is to output a high-accuracy approximation of the uniform distribution over the space of satisfying assignments for f. This distribution learning problem may be viewed as a demanding variant of standard Boolean function learning, where the learning algorithm only receives positive examples and --- more importantly --- must output a hypothesis function which has small *multiplicative* error (i.e. small error relative to the size of f^{-1}(1).
<p>
As our main results, we show that the two most widely studied classes of Boolean functions in computational learning theory --- linear threshold functions and DNF formulas --- have efficient distribution learning algorithms in our model. Our algorithm for linear threshold functions runs in time poly(n,1/epsilon) and our algorithm for polynomial-size DNF runs in quasipolynomial time.  On the other hand, we also prove complementary hardness results which show that under cryptographic assumptions, learning monotone 2-CNFs, intersections of 2 halfspaces, and degree-2 PTFs are all hard. This shows that our algorithms are close to the limits of what is efficiently learnable in this model.
<p>
Joint work with Anindya De and Ilias Diakonikolas.
</div>
</li>



<li>
Friday, November 29, 2013: 
<b>Thanksgiving, no seminar.</b> 
</li>



<li>
Friday, November 22, 2013: 

<b>
<a class="name" href="http://people.csail.mit.edu/eblais/">
Eric Blais</a></b> (MIT, CS), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Approximating Boolean Functions with Depth-2 Circuits</a>
[<a rel="nofollow" target="_blank" href="files/theory/eric.pdf"><font color="green"><b>Slides</b></font></a>]
<div class="abs">
<p> 
A common theme in many areas of theoretical computer science is that it is sometimes much easier to compute approximations to a function than to compute the function exactly, while sometimes approximation does not make things any easier. The study of this general topic in different settings has led to many deep and far-reaching consequences, including for example much recent progress on the P vs. NP problem and the development of sublinear algorithms.
<p>
In this talk, we explore how approximation affects the circuit complexity of boolean functions. Specifically, we restrict our attention to depth-2 circuits (i.e., DNFs and CNFs) and we ask how large such a circuit must be to agree with a given target function on 99% of all possible inputs. As we will see during the talk, the study of this question leads to surprising results and to interesting connections to the design of randomized algorithms, to information theory, to the analysis of boolean functions, and to covering codes.
<p>
Joint work with Li-Yang Tan.
</div>
</li>

<li>
Friday, November 15, 2013: 

<b>
<a class="name" href="http://researcher.watson.ibm.com/researcher/view.php?person=us-dpwoodru">
David Woodruff</a></b> (IBM Research, Almaden), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Sketching as a Tool for Numerical Linear Algebra</a>
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/david.ppt"><font color="green"><b>Slides</b></font></a>]
<div class="abs">
<p>
I will discuss how sketching techniques from the data stream literature can be used to speed up well-studied algorithms for problems occurring in numerical linear algebra, such as least squares regression and approximate singular value decomposition. I will also discuss how they can be used to achieve very efficient algorithms for variants of these problems, such as robust and structured regression. In many cases the algorithms obtained using sketching are the only known ways to obtain asymptotically optimal algorithms.</div>
</li>

<li>
<font color="red"><b>Tuesday</b></font>, November 12, 2013, <font color="red"><b>11 am</b></font>: 

<b>
<a class="name" href="http://www.cse.wustl.edu/~kunal/">
Kunal Agrawal</a></b> (Washington University, St. Louis), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">Provably Good Schedulers for Parallel Computations</a>
<div class="abs">
<p>
In recent years, parallel computing has become
ubiquitous, as modern computation platforms, from smartphones to
network routers and personal computers to large clusters and clouds,
each contain multiple processors.  On these parallel computers,
scheduling decisions --- what happens when and
where --- has a large impact on performance.  This talk will present recent research on
scheduling algorithms that provide provable guarantees on the
performance parallel programs that they schedule.  In particular, the talk will cover 2 main topics: 
(1) Data structures are an important component in designing sequential algorithms.  However, using 
data structures within parallel algorithms while guaranteeing speedup is challenging due to contention between
concurrent operations and the resulting slowdown.  Amortized data structures, in particular, are extremely difficult
to use from within parallel algorithms.  This talk will present BATCHER, a scheduling algorithm that allows
parallel algorithms to use data structures while still guaranteeing speedup to the algorithm.  
(2) Streaming computations represent an important class of parallel applications.  We consider the problem
of scheduling streaming applications to minimize the number of cache misses incurred by the application.
We show that for a large class of streaming applications, we can reduce this problem to a graph partitioning
problem.  
</div>
</li>

<li>
Friday, November 08, 2013:
<b><a class="name" href="http://research.microsoft.com/en-us/people/siuonc/">Siu On Chan</a></b> (Microsoft Research, New England), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Approximate Constraint Satisfaction Requires Large LP Relaxations</a>
<div class="abs">
<p>
We prove super-polynomial lower bounds on the size of linear programming relaxations for approximation versions of constraint satisfaction problems. We show that for these problems, polynomial-sized linear programs are exactly as powerful as programs arising from a constant number of rounds of the Sherali-Adams hierarchy.
<p>
In particular, any polynomial-sized linear program for Max Cut has an integrality gap of 1/2 and any such linear program for Max 3-Sat has an integrality gap of 7/8.
<p>
Joint work with James Lee, Prasad Raghavendra, and David Steurer.
</div>
</li>

<li>
Friday, November 01, 2013: 

<b>
<a class="name" href="http://grigory.us">
Grigory Yaroslavtsev</a></b>, (Brown University, ICERM) 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Testing Properties Under L<sub>p</sub> Distances</a>
<div class="abs">
<p>We present sublinear algorithms for approximately testing properties of real-valued data under L<sub>p</sub> distance measures (for p = 1,2). Our algorithms allow one to distinguish datasets which have a certain property from datasets which are far from having it with respect to L<sub>p</sub> distance. While the classical property testing framework developed under the Hamming distance has been studied extensively, testing under L<sub>p</sub> distances has received little attention. 
<p>
For applications involving noisy real-valued data using L<sub>p</sub> distances is natural because unlike Hamming distance it allows to suppress distortion introduced by the noise.  Moreover, we show that it also allows one to design simple and fast algorithms for classic problems, such as testing monotonicity, convexity and Lipschitz conditions (also known as “sensitivity”). Our algorithms require minimal assumptions on the choice of the sampled data (either uniform or easily samplable random points suffice). We also show connections between our L<sub>p</sub>-testing model and the standard framework of property testing under the Hamming distance. In particular, some of our results improve existing bounds for Hamming distance.
<p>
Joint work with Piotr Berman and Sofya Raskhodnikova.
</div>
<li>
Friday, October 25, 2013: 

<b>
<a class="name" href="http://cs.brown.edu/people/pvaliant/">
Paul Valiant
</a></b>(Brown University, CS), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">Instance-by-Instance Optimal Identity Testing</a>
<div class="abs">
<p>

The amount of data being routinely processed by computers is exploding, and this puts a new demand on the algorithms community: do our algorithms make efficient use of their data? Many familiar problems can be reexamined in this light to yield significant and practical improvements. A small asymptotic algorithmic improvement, from needing terabytes of data to merely gigabytes, effectively cut costs by a factor of a thousand in a data-driven world.
<p>
In this talk we consider a fundamental hypothesis testing problem: given data from a probability distribution, was it drawn from a known distribution P, or from a distribution far from P? Our analysis reveals several distinct features that previous algorithms failed to take advantage of, and culminates by showing that these are essentially all the possible features an algorithm could take advantage of. Namely, we introduce what we call an "instance-by-instance optimal" algorithm and show that our algorithm, when testing against an input distribution P, performs within constant factors of the best possible performance of the best possible algorithm customized for P.
<p>
Our analyses introduces a new way of using linear programming duality to automatically analyze a broad class of inequalities that generalizes Cauchy-Schwarz, Holder's inequality, and the monotonicity of L<sub>p</sub> norms.

<p>
Joint work with Gregory Valiant
</li>


<li>
Friday, October 18, 2013: 
 
<b>
<a class="name" href="http://www.ece.umd.edu/~cpap">
Charalampos Papamanthou</a></b> (University of Maryland, College Park), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Practical Dynamic Proofs of Retrievability
</a>
<div class="abs">
<p>
Proofs of Retrievability (PoR) enable a client to store a number of file blocks with a cloud
server so that later the server can prove possession of all the
data in a very efficient manner (i.e., with constant computation and bandwidth). Although many efficient PoR schemes for static data have been constructed, only two dynamic PoR
schemes exist, which have various disadvantages such as large client storage or practical inefficiency.
<p>
We propose the first dynamic POR scheme that is both theoretically and practically efficient. Specifically (i) all complexity measures involved are at most logarithmic in the number of the outsourced blocks; (ii) our system implementation is very practical.
<p>
Joint work with Elaine Shi and Emil Stefanov, to appear at CCS 2013
</div>
</li>



<li>
Friday, October 11, 2013: 

<b>
<a class="name" href="http://www.cse.psu.edu/~sofya/">
Sofya Raskhodnikova</a></b> (Penn State, Boston University), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Private Analysis of Graphs
</a>
[<a rel="nofollow" target="_blank" href="files/theory/sofya.pptx"><font color="green"><b>Slides</b></font></a>]
<div class="abs">
<p>
We discuss algorithms for the private analysis of network data. Such algorithms work on data sets that contain sensitive relationship information (for example, romantic ties). Their goal is to compute approximations to global statistics of the graph while protecting information specific to individuals. Our algorithms satisfy a rigorous notion of privacy, called node differential privacy. Intuitively, it means that an algorithm's output distribution does not change significantly when a node and all its adjacent edges are removed from a graph. We present several techniques for designing node differentially private algorithms. We also develop methodology for analyzing the accuracy of such algorithms on realistic networks. Our techniques are based on combinatorial analysis, network flow, and linear and convex programming.

Based on joint work with Shiva Kasiviswanathan, Kobbi Nissim and Adam Smith
</div>
</li>


<li>
Friday, October 04, 2013: 

<b>
<a class="name" href="http://people.seas.harvard.edu/~karthe/">
Karthekeyan Chandrasekaran</a></b> (Harvard SEAS), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
A Polynomial-Time Cutting Plane Algorithm for Matching
</a>
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/karthik.pptx"><font color="green"><b>Slides</b></font></a>]
<div class="abs">
<p>
The cutting plane approach to optimal matchings has been discussed by several authors over the past decades [Padberg-Rao, Grotschel-Holland, Lovasz-Plummer, Trick, Fischetti-Lodi], and its convergence has been an open question. We give a cutting plane algorithm for the minimum-cost perfect matching problem using Edmonds' blossom inequalities as cuts and prove polynomial convergence of this algorithm.
<p>
Our main insight is an LP-based method to retain/drop candidate cutting planes. Our cut-addition is based on maintaining laminarity. This leads to a sequence of intermediate linear programs with a linear number of constraints whose optima are half-integral and supported by a disjoint union of odd cycles and edges. This structural property of the optima is instrumental in finding violated blossom inequalities (cuts) in linear time. The number of cycles in the support of the half-integral optima acts as a potential function to show efficient convergence to an integral solution.
<p>
Joint work with Laszlo Vegh and Santosh Vempala.
</div>
</li>


<li>
Friday, September 27, 2013: 

<b>
<a class="name" href="http://people.seas.harvard.edu/~minilek">
Jelani Nelson</a></b> (Harvard SEAS), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
OSNAP: Faster numerical linear algebra algorithms via sparser
subspace embeddings
</a>
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/jelani.pdf"><font color="green"><b>Slides</b></font></a>]
<div class="abs">
<p>
An "oblivious subspace embedding" (OSE) is a distribution
over matrices S such that for any low-dimensional subspace V, with
high probability over the choice of S, ||Sx||_2 approximately equals
||x||_2 (up to 1+eps multiplicative error) for all x in V
simultaneously. Sarlos in 2006 pioneered the use of OSE's for speeding
up algorithms for several numerical linear algebra problems. Problems
that benefit from OSE's include: approximate least squares regression,
low-rank approximation, l_p regression, approximating leverage scores,
and constructing good preconditioners.
<p>
We give a class of OSE distributions we call "oblivious sparse
norm-approximating projections" (OSNAP) that yield matrices S with few
rows that are also extremely sparse, yielding improvements over recent
work in this area by (Clarkson, Woodruff STOC '13). In particular, we
show S can have O(d^2) rows and 1 non-zero entry per column, or even
O(d^{1+gamma}) rows and poly(1/gamma) non-zero entries per column for
any desired constant gamma>0. When applying the latter bound for
example to the approximate least squares regression problem of finding
x to minimize ||Ax - b||_2 up to a constant factor, where A is n x d
for n >> d, we obtain an algorithm with running time O(nnz(A) +
d^{omega + gamma}). Here nnz(A) is the number of non-zero entries in
A, and omega is the exponent of square matrix multiplication.
<p>
Our main technical result is essentially a Bai-Yin type theorem in
random matrix theory and is likely to be of independent interest: i.e.
 we show that for any U in R^{n x d} with orthonormal columns and
 random sparse S with appropriately chosen entries and sufficiently
 many rows, all singular values of SU lie in the interval [1-eps,
 1+eps] with good probability.
<p>
 Joint work with Huy Lê Nguyễn (Princeton).
</div>
</li>

<li>
Friday, September 20, 2013:

<b>
<a class="name" href="http://www.cs.cmu.edu/~yangp">
Richard Peng</a></b> (MIT Math), 
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title">
Iterative Row Sampling
</a> 
[<a rel="nofollow" target="_blank" href="http://grigory.us/files/theory/rpeng.pptx"><font color="green"><b>Slides</b></font></a>]
<div class="abs">

There has been significant interest and progress recently in algorithms that solve regression problems involving tall and thin matrices in input sparsity time. Given a n * d matrix where n >> d, these algorithms find an approximation with fewer rows, allowing one to solve a poly(d) sized problem instead. In practice, the best performances are often obtained by invoking these routines in an iterative fashion. We show these iterative methods can be adapted to give theoretical guarantees comparable to and better than the current state of the art.

<p>

 Our approaches are based on computing the importance of the rows, known as leverage scores, in an iterative manner. We show that alternating between computing a short matrix estimate and finding more accurate approximate leverage scores leads to a series of geometrically smaller instances. This gives an algorithm whose runtime is input sparsity plus an overhead comparable to the cost of solving a regression problem on the smaller approximation. Our results build upon the close connection between randomized matrix algorithms, iterative methods, and graph sparsification.
</div>
</li>

<li> Friday, September 13, 2013: No seminar. 
</li>


<li>
Friday, September 06, 2013:

<b>
<a class="name" href="http://grigory.us">
Grigory Yaroslavtsev</a></b>
(Brown <a rel="nofollow" target="_blank" href="http://icerm.brown.edu">ICERM</a>),
<br><a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title"> Kickoff meeting
</a>
<div class="abs">
We will discuss the plan for the semester, pick time, etc.
The meeting will be 15-20 minutes.
</div>
</li>
-->
</ul>
</a>

<a id="archives">
<h3>Archives:</h3></a>
<ul>
<li><a rel="nofollow" target="_blank" href="http://grigory.us/theory-seminar-brown-fall13.html">Fall 2013</a></li>
<li>Previous years: <a rel="nofollow" target="_blank" href="http://cs.brown.edu/~matteo/theorylunch.html">CS Theory Lunch</a></li> 

</ul>
<!--
<a id="calendar"></a>
<p></p>
<p></p>

<iframe src="https://www.google.com/calendar/embed?mode=AGENDA&amp;height=600&amp;wkst=1&amp;bgcolor=%23FFFFFF&amp;src=s1nrtsj2ekjdr6aakijq49stkc%40group.calendar.google.com&amp;color=%23125A12&amp;ctz=America%2FNew_York" style=" border-width:0 " width="800" height="600" frameborder="0" scrolling="no"></iframe>
		 <br/>
</a>
-->
Parts of this website were designed by <a rel="nofollow" target="_blank" href="http://thl.epfl.ch/madry">Aleksander M&#261dry</a> and <a rel="nofollow" target="_blank" href="http://www.mit.edu/~ecprice">Eric Price</a> at MIT. Currently it is maintained by <a rel="nofollow" target="_blank" href="http://grigory.us">Grigory Yaroslavtsev</a>.

</div>
</div>

</body></html>


