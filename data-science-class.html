<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>

<meta content="text/html; charset=UTF-8" http-equiv="content-type">
<meta property="og:image" content="http://grigory.us/pics/b609-poster-homepage.png">
<meta property="og:image:type" content="image/png">

<title>CSCI-B609: Foundations of Data Science</title>

<link rel="stylesheet" href="css/font-awesome-4.1.0/css/font-awesome.min.css">

<!-- Bootstrap core CSS -->
<link href="css/bootstrap.css" rel="stylesheet">

<!-- Custom styles for this template -->
<link href="iu-template.css" rel="stylesheet">
<link href="list-template.css" rel="stylesheet">


<style type="text/css">
.title {font-weight: bold; }
.abs {
background-color: #eee;
padding: 5px;
display: none;
}
</style>

<script type="text/javascript">
function showAbstract(e){
f = e;
var div;
for(div = e.nextSibling; div.className != "abs"; div = div.nextSibling);

if (div.style.display=="block"){
div.style.display="";
} else {
div.style.display="block";
}
return true;
}
</script>


</head>

<body>
<div class="navbar navbar-inverse navbar-fixed-top navbar-default" role="navigation">
<div class="container">
<div class="navbar-header">
<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
<span class="sr-only">Toggle navigation
<span class="icon-bar">
<span class="icon-bar">
<span class="icon-bar">
</button>
<a class="navbar-brand" href="#"><b>Foundations of Data Science</b></a> 
<a class="navbar-brand" href="#sketch">Abstract</a>
<a class="navbar-brand" href="#lectures">Lectures</a>
<a class="navbar-brand" href="#hw">Homework</a>
<a class="navbar-brand" href="#project">Project</a>
</div>
</div>
</div>

<div class="container">

<div class="iu-template">
<br/>
<br/>
<br/>

<h1 style="padding-top:0pt;margin-top:0pt;"><font color="brown">CSCI-B609: Foundations of Data Science</font>
</h1>

<!--<a id="information"><h1>Information</h1></a>
-->
<table>
<tr>	
<td>
	<img src="pics/b609-poster-homepage.png" width="225px" style="border:1px solid black">
</td>
<td>	
<ul class="fa-ul">
	<li> <i class="fa-li fa fa-question"></i>  <b>What?</b> This class will give you a mathematical toolkit that can be applied to problems in data anslysis. Target audience are students interested in algorithms, statistics, machine learning, data mining and related areas. 
	<li> <i class="fa-li fa fa-question"></i>  <b>Who?</b> <b><a href="http://grigory.us">Grigory Yaroslavtsev</a></b>.
	<li> <i class="fa-li fa fa-question"></i>  <b>When?</b> Fall 2016, MW 16:00 &ndash; 17:15. 
	<li> <i class="fa-li fa fa-question"></i>  <b>Where?</b> Ballantine Hall, 310. 
<li> <i class="fa-li fa fa-question"></i> <b>Need permission?</b> Please, send me an e-mail with relevant coursework you have taken. In most cases permission will be granted.
<li> <i class="fa-li fa fa-question"></i> <b>Grading?</b> The class will be graded based on home assignments and a final project.
<li> <i class="fa-li fa fa-question"></i> <b>TA?</b> <b><a href="https://www.linkedin.com/in/md-lisul-islam-rizve-7b24135b">Md. Lisul Islam</a></b>, IU id: islammdl. 
<li> <i class="fa-li fa fa-question"></i> <b>Office hours?</b> Held by the Md. Lisul Islam on Tuesdays, 12:30&ndash;2pm at Lindley Hall, 125.
<li> <i class="fa-li fa fa-question"></i> <b>Prerequisites?</b> There are no formal prerequisites, but you should be familiar with the basics of algorithm design and analysis, discrete mathematics, probability and have a strong mathematical background.
<li> <i class="fa-li fa fa-question"></i> <b>Textbook?</b> 
This class is primarily based on the emponimous textbook by Blum, Hopcroft and Kannan which is <a href="./files/bhk-book.pdf">available here</a>.
<li> <i class="fa-li fa fa-question"></i> <b>More questions?</b> 
In addition to Canvas we are using Piazza for questons. Sign up: <a href="https://piazza.com/iu/fall2016/b609">here</a>. <b>Please, don't use my personal e-mail for questions.</b>
</ul>				     
</td>
</tr>
<table>

<a id="sketch"><h1>Abstract</span>   </h1></a>

<p>
<p>
This class covers a rapidly evolving area.
Related classes at other universities:
<ul class="fa-ul">
	<li><i class="fa-li fa fa-institution"></i><a rel="nofollow" target="_blank" href="http://www.cs.cornell.edu/Courses/cs4850/2015sp/">Mathematical Foundations for the Information Age</a> by John Hopcroft at Cornell
	<li><i class="fa-li fa fa-institution"></i><a rel="nofollow" target="_blank" href="https://barnasaha.net/barna/compsci590d/">Algorithms for Data Science</a> by Barna Saha at University of Massachusetts, Amherst
	<li><i class="fa-li fa fa-institution"></i><a rel="nofollow" target="_blank" href="https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/">Computer Science Theory for the Information Age</a> by Venkatesan Guruswami and Ravi Kannan at CMU 
	
</ul>	


<a id="lectures"> <h1>Lectures</h1></a>

<ul class="fa-ul lectures">
	        <li><i class="fa-li fa fa-group"></i> <b>Week 1.</b> [Slides (<a href="./b609/lec01.pptx">pptx</a>, <a href="./b609/lec01.pdf">pdf</a>)]
		        Introduction. Probablity basics. Probabilistic inequalities and concentration bounds.
	        <li><i class="fa-li fa fa-group"></i> <b>Week 2.</b> [Slides (<a href="./b609/lec02.pptx">pptx</a>, <a href="./b609/lec02.pdf">pdf</a>)]
			Properties of high-dimensional space. Surface area and volume of the unit ball in high dimensions, distribution of volume.
			Near-orthogonality of random high-dimensional vectors. Sampling from a unit ball. Gaussian concentration. 
	        <li><i class="fa-li fa fa-group"></i> <b>Week 3.</b> [Slides (<a href="./b609/lec03.pptx">pptx</a>, <a href="./b609/lec03.pdf">pdf</a>)]
			Nearest neighbor search and random projections. Separating high-dimensional Gaussians. Fitting a high-dimensional Gaussian (maximum likelihood).
		<li><i class="fa-li fa fa-group"></i> <b>Week 4.</b> [Slides (<a href="./b609/lec04.pptx">pptx</a>, <a href="./b609/lec04.pdf">pdf</a>)]
			Singular Value Decomposition and Best-Fit Subspaces. Greedy construction of the SVD. Low-rank approximation (Frobenius and spectral norm). Power method. 
		<li><i class="fa-li fa fa-group"></i> <b>Week 5.</b> [Slides (<a href="./b609/lec05.pptx">pptx</a>, <a href="./b609/lec05.pdf">pdf</a>)]
			Faster power method. Applications of SVD (centering data for SVD and PCA). Separating mixtures of Gaussians. HITS and document ranking.
		<li><i class="fa-li fa fa-group"></i> <b>Week 6.</b> [Slides (<a href="./b609/lec06.pptx">pptx</a>, <a href="./b609/lec06.pdf">pdf</a>)]
			Random walks and Markov chains. Stationary distribution. Fundamental theorem of Markov chains. Introduction to machine learning: distributional batch setting and uniform convergence. Online learning and perceptron algorithm.	
		<li><i class="fa-li fa fa-group"></i> <b>Week 7.</b> [Slides (<a href="./b609/lec07.pptx">pptx</a>, <a href="./b609/lec07.pdf">pdf</a>)]
		VC-dimension and Sauer's lemma. VC-dimension of various classes and proof of the VC-theorem.
</ul>




<a id="hw"> <h1>Homework</h1></a>
<br>

<ul class="fa-ul homework">
	        <li><i class="fa-li fa fa-file-text-o"></i> <b>Homework 1.</b> (due September 25, 11:59 pm EST) [Source (<a href="./b609/hw1.tex">tex</a>), Compiled (<a href="./b609/hw1.pdf">pdf</a>)]
	        <li><i class="fa-li fa fa-file-text-o"></i> <b>Homework 2.</b> (due October 17, 11:59 pm EST) [Source (<a href="./b609/hw2.tex">tex</a>), Compiled (<a href="./b609/hw2.pdf">pdf</a>)]
		
</ul>

<a id="project"> <h1>Project</h1></a>
<br>
Details about the project: [Slides (<a href="./b609/project.pptx">pptx</a>, <a href="./b609/project.pdf">pdf</a>)].
Key dates:
<ul>
	<li> 1-page project proposal due on October 31, 11:59 EST. 
	<li> Full project submission due on December 09, 11:59 EST.
</ul>

<br>
<br>



</body></html>


