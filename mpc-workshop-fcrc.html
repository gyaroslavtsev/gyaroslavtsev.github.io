
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
<meta property="og:title" content="Sublinear Day">
<meta property="og:description" content="Sublinear Algorithms and Big Data Day">
<meta property="og:image" content="http://grigory.github.io/pics/mpc/logo-big.png">
<meta property="og:url" content="http://grigory.github.io/mpc-workshop-fcrc.html">
<meta property="og:image:type" content="image/png">
<meta property="og:image:width" content="1000"/> 
<meta property="og:image:height" content="1000" />





<meta content="text/html; charset=UTF-8" http-equiv="content-type">
<title>Algorithmic Frontiers of Modern Massively Parallel Computation</title>

<link href="css/bootstrap.css" rel="stylesheet">
<link href="css/social-buttons.css" rel="stylesheet">
<link href="starter-template.css" rel="stylesheet"> 
<link rel="stylesheet" href="files/font-awesome-4.0.3/css/font-awesome.min.css">
<link rel="image_src" href="pics/mpc-logo.png" />

<!-- Bootstrap core CSS -->
<link href="css/bootstrap.css" rel="stylesheet">

<!-- Custom styles for this template -->
<link href="starter-template.css" rel="stylesheet">


<link rel="image_src" href="pics/mpc/logo.png">


<style type="text/css">
.title {font-weight: bold; }
.abs {
background-color: #eee;
padding: 5px;
display: none;
}
</style>


<script type="text/javascript">
function showAbstract(e){
f = e;
var div;
for(div = e.nextSibling; div.className != "abs"; div = div.nextSibling);

if (div.style.display=="block"){
div.style.display="";
} else {
div.style.display="block";
}
return true;
}
</script>

<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>


</head>

<body id="mpc-home" class="text-center homepage">

<div id="fb-root"></div>
<script>(function(d, s, id) {
var js, fjs = d.getElementsByTagName(s)[0];
if (d.getElementById(id)) return;
js = d.createElement(s); js.id = id;
js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>


<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
<div class="container">
<div class="navbar-header">
<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
<span class="sr-only">Toggle navigation
<span class="icon-bar">
<span class="icon-bar">
<span class="icon-bar">
</button>
<a class="navbar-brand" href="#"><b>Algorithmic Frontiers of Modern Massively Parallel Computation</b></a>
<a class="navbar-brand" href="#info">Information</a>
<a class="navbar-brand" href="#schedule">Schedule</a>
<a class="navbar-brand" href="#speakers">Speakers</a>
<a class="navbar-brand" href="#orgs">Orgs & Support</a>
</div>
</div>
</div>

<div class="container">

<div class="starter-template">
<br/>
<br/>
<br/>

<!--
<h1 style="padding-top:0pt;margin-top:0pt;"><img src="img/sublinear.jpeg" style="width:50pt"> <font color="brown">April 18: Sublinear Day</font>
</h1>
-->



<a id="info"><h1>Information</h1></a>
This workshop will bring together researchers in theoretical foundations of parallel and distributed computing for a discussion of recent progress and directions for future research in modeling and algorithms for modern massively parallel computational systems (such as Hadoop/MapReduce/Storm/Spark, etc.) The workshop will be primarily focused on theoretical aspects of such systems, including a comprehensive overview of their power and limitations. It will bring an uninitiated researcher from the STOC and SPAA community up to speed with the key results and challenges in the area, while also illustrating connections with other established areas in massive data processing (streaming, sublinear algorithms, etc.) We also expect this workshop to be of interest to a broader audience and in particular to researchers working in related areas where algorithms for big data are known to be impactful: machine learning, optimization, image and streaming data processing, etc.
<p>


<ul style="list-style:none;">
<li> <b>When?</b> <br>09:00 &ndash; 17:00, Sunday, June 14, 2015.
<li> <b>Where?</b> <br> <a href="http://fcrc.acm.org/ ">ACM FCRC 2015</a>, Oregon Convention Center, Portland, OR. 
<li> <b>What?</b> <br> See the <a href="#schedule">schedule</a> and the <a href="#speakers">list of speakers</a>.
<li> <b>Background?</b> We will welcome researchers who are interested in theoretical foundations of distributed computing regardless of the background.
No prior knowledge is required, although we recommend to read <a href="http://grigory.us/blog/massively-parallel-universes/ ">this blog post</a> for a gentle introduction to theoretical modeling of massively parallel computation. 
<li> <b>Do I need to register?</b> <br> No separate registration is required for the workshop. 
</ul>				     

<br>

<div class="fb-like" data-href="http://grigory.github.io/mpc-workshop-fcrc.html" data-layout="button_count" data-action="like" data-show-faces="true" data-share="true"></div>

&nbsp;
&nbsp;
&nbsp;
&nbsp;

<a href="https://twitter.com/share" class="twitter-share-button" data-url="http://grigory.github.io/mpc-workshop-fcrc.html" data-via="gyaroslavtsev" align="top">Tweet</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>

<!-- Place this tag where you want the +1 button to render. -->
<div class="g-plusone" data-annotation="inline" data-width="100" data-href="http://grigory.github.io/mpc-workshop-fcrc.html" align="top"></div>




<!-- Place this tag after the last +1 button tag. -->
<script type="text/javascript">
(function() {
var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
po.src = 'https://apis.google.com/js/platform.js';
var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
})();
</script>

</p>

<a id="schedule"> <h1> Schedule</h1></a>
<ul class="fa-ul">
<li> <i class="fa li fa fa-group"></i> 
09:00 &ndash; 09:45
<p> <b>Introduction</b>
<p>

<li> <i class="fa li fa fa-group"></i>
09:45&ndash; 10:30
<br>
<a class="name" href=" http://onak.pl " style="text-decoration: none"><b>Krzysztof Onak</b></a> (IBM T.J. Watson Research Center)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title" style="text-decoration: none">
Parallel Algorithms for Graphs on a Very Large Number of Nodes
</a>
<div class="abs">
<p align="left">
 The research on parallel algorithms for modern massive computation systems has identified minimizing the number of computation rounds as one of the main challenges. In the case of graph algorithms, it has encountered a natural barrier in the form of the connectivity problem. This problem is widely believed to require a large, super-constant number of computation rounds if the number of nodes is significantly larger than the amount of memory of a single machine. Moreover, a lower bound on the number of computation rounds for connectivity would imply a similar lower bound for a number of graph problems.
<br>
 I will present a few graph algorithms that require a constant number of computation rounds as long as the number of nodes is at most polynomially larger than the amount of memory of a single machine. The algorithms either take advantage of a natural assumption on the input data or solve a slightly relaxed version of the problem. The set of applied techniques ranges from geometric random partitioning to graph exploration methods developed for sublinear-time algorithms.
 <br>
 (The talk will include results obtained in collaboration with Alexandr Andoni, Jakub Łącki, Aleksander Mądry, Slobodan Mitrović, Aleksandar Nikolov, Piotr Sankowski, and Grigory Yaroslavtsev.)

</p>
</div>
</li>
<p>

<li> <i class="fa li fa fa-group"></i>
10:30&ndash; 11:00
<br><a class="name" href="https://sites.google.com/site/ravik53/ "  style="text-decoration: none" ><b>Ravi Kumar</b></a> (Google Research, Mountain View)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title"  style="text-decoration: none">
Scalable Correlation Clustering
</a>
<div class="abs">
<p align="left">
The goal in correlation clustering is, given a graph with signed
edges, cluster the nodes to minimize the number of disagreements.
This is a well-studied problem with applications in machine learning
and social network analysis.  We present a new approximation algorithm
for correlation clustering that is easily implementable in
computational models such as MapReduce and streaming, and runs in a
small number of rounds.
<br>
Joint work with Flavio Chierichetti and Nilesh Dalvi
</p>
</div>
<p>

<li> <i class="fa li fa fa-coffee"></i> 
11:00 &ndash; 11:30
<p> <b>Coffee Break</b>
<p>

<li> <i class="fa li fa fa-group"></i>
11:30 &ndash; 12:15
<br> <a class="name" href="http://www.cs.cmu.edu/~ninamf/ "  style="text-decoration: none" ><b>Nina Balcan</b></a> (Carnegie Mellon University)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title" style="text-decoration: none" >
Distributed Machine Learning
</a>
<div class="abs">
<p align="left">
<br>
We consider the problem of learning from distributed data and analyze
fundamental algorithmic and communication complexity questions involved.
Broadly, we consider a framework where information is distributed between
several locations, and our goal is to learn a low-error hypothesis with
respect to the overall  data by  using as little communication, and as few
rounds of communication, as possible. As an example, suppose k research
groups around the world have collected large scientific datasets, such as
genomic sequence data or sky survey data, and we wish to perform learning
over the union of all these different datasets without too much
communication.
<br>
In this talk, I will first discuss a general statistical or PAC style
framework for analyzing communication complexity issues involved when
doing distributed supervised machine learning, i.e., learning from
annotated data distributed across multiple locations. I will discuss
general lower bounds on the amount of communication needed to learn a
given class and broadly-applicable techniques for achieving
communication-efficient learning, as well as efficient learning algorithms
with especially good communication performance for specific interesting
classes.
<br>
I will also discuss algorithms with good communication complexity for
unsupervised learning and dimensionality reduction problems, with
interesting connections to efficient distributed coreset construction.
</p>
</div>
<p>

<li> <i class="fa li fa fa-cutlery"></i> 
&#160;12:15 &ndash; 14:00
<p> <b>Lunch Break</b>
<p>

<li> <i class="fa li fa fa-group"></i>
14:00 &ndash; 14:45 
<br><a class="name" href="http://homes.cs.washington.edu/~beame/beame.html" style="text-decoration: none"   ><b>Paul Beame</b></a> (University of Washington)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title"  style="text-decoration: none" >
TBD
</a>
<div class="abs">
<p align="left">
TBD
</p>
</div>
<p>


<li> <i class="fa li fa fa-group"></i>
14:45 &ndash; 15:30 
<br><a class="name" href="http://people.csail.mit.edu/mirrokni/Welcome.html "  style="text-decoration: none" ><b>Vahab Mirrokni</b></a> (Google Research, NYC)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title"  style="text-decoration: none">
Randomized Composable Core-sets for Distributed Computation
</a>
<div class="abs">
<p align="left">
An effective technique for solving optimization problems over massive data sets is to partition the data into smaller pieces, solve the problem on each piece and compute a representative solution from it, and finally obtain a solution inside the union of the representative solutions for all pieces. Such an algorithm can be implemented easily in 2 rounds of MapReduces or be applied in an streaming model. This technique can be captured via the concept of {\em composable core-sets}, and has been recently applied to solve diversity maximization problems as well as several clustering problems. However, for coverage and submodular maximization problems, impossibility bounds are known for this technique. In this talk, after a initial discussion about this technique and applications in diversity maximization and clustering problems, we focus on the submodular maximization problem, and show how to apply a randomized variant of composable core-set problem, and achieve 1/3-approximation for monotone and non-montone submodualr maximization problems. We prove this result by applying a simple greedy algorithm and show that a large class of algorithms can be deployed in this framework. Time-permitting, we show a more involved algorithm that achieves 54% of the optimum in two rounds of MapReduces.
<br>
The main part of the talk is to appear in STOC 2015 and is a joint work with Morteza ZadiMoghaddam. The initial parts are from two recent papers that appeared in PODS 2014 and NIPS 2014.
</p>
</div>
<p>

<li> <i class="fa li fa fa-coffee"></i> 
15:30 &ndash; 16:00
<p> <b>Coffee Break</b>
<p>

<li> <i class="fa li fa fa-group"></i>
16:00 &ndash; 16:45 
<br> <a class="name" href="http://research.engineering.wustl.edu/~bmoseley/  "  style="text-decoration: none"  ><b>Benjamin Moseley</b></a> (Washington University, St. Louis)
<br>
<a rel="nofollow" target="_blank" href="javascript:void(0)" onclick="showAbstract(this)" class="title"  style="text-decoration: none"  >
Sample and Prune: An Efficient MapReduce Method for Submodular Optimization
</a>
<div class="abs">
<p align="left">
MapReduce has been widely considered for optimizing submodular functions over large data sets and several techniques have been developed.  In this talk, we will discuss the Sample and Prune procedure.  Sample and Prune is a distributed sampling method used to efficiency discover a small set of representative elements from a large data set.  We discuss how Sample and Prune can be used for submodular optimization in MapReduce.  In particular, we show how this procedure can be utilized to simulate a class of greedy sequential algorithms for submodular optimization in MapReduce. We will further discuss its use to construct new distributed algorithms for submodular optimization and how it could possibly be extended to construct efficient algorithms for a wide range of problems in the distributed setting.
</p>
</div>
<p>

<li> <i class="fa li fa fa-group"></i> 
16:45 &ndash; 17:00 
<br><b>Q&A + Discussion</b>


</ul>

<a id="speakers"><h1>Speakers</h1></a>
<div>
<ul class="list-thumbnail">


<li>
<img alt=""  src="pics/mpc/nina.jpg">
<h4>Nina Balcan (Carnegie Mellon University)</h4> 
<p align="left">

Maria Florina Balcan is an Associate Professor in the School of Computer
Science at Carnegie Mellon University. Her main research interests are
machine learning, computational aspects in economics and game theory, and
algorithms. Her honors include the CMU SCS Distinguished Dissertation
Award, an NSF CAREER Award, a Microsoft Faculty Research Fellowship, a
Sloan Research Fellowship, and several paper awards. She is currently a
board member of the International Machine Learning Society and was
recently Program Committee Chair for COLT 2014.


</p>

<li>
<img alt=""  src="pics/mpc/paul.jpg">
<h4>Paul Beame (University of Washington)</h4> 
<p align="left">

Paul Beame is a Professor in the Department of Computer Science &amp; Engineering at the University of Washington.

Paul received his B.Sc. in Mathematics in 1981, an M.Sc. in Computer Science in 1982, and Ph.D. in Computer Science in 1987, all from the University of Toronto. He was a Post-doctoral Research Associate at M.I.T. for the 1986-87 academic year and joined the University of Washington in 1987.

Paul's research is concerned primarily with computational complexity. His main interest is in proving lower bounds on the resources needed for solving computational problems. Such topics include communication complexity, time-space tradeoff lower bounds, proof complexity, and data structures. In addition, Paul is interested in problems related to formal reasoning and verification. He has worked on the application and extension of the techniques of symbolic model checking for the verification of software specifications.

</p>

<li>
<img alt=""  src="pics/mpc/ravi.jpg">
<h4>Ravi Kumar (Google Research, Mountain View)</h4> 
<p align="left">
Ravi Kumar has been a senior staff research scientist in Google since 2012. Prior to this, he was a research staff member at the IBM Almaden Research Center and a principal research scientist at Yahoo! He obtained his PhD in Computer Science from Cornell University in 1998. His primary interests are web and data mining, social networks, algorithms for large data sets, and theory of computation.
</p>

<li>
<img alt=""  src="pics/mpc/vahab.jpg">
<h4>Vahab Mirrokni (Google Research, NYC)</h4> 
<p align="left">
Vahab Mirrokni is a Senior Staff Research Scientist, heading the algorithms research group at Google Research, New York. He received his PhD from MIT in 2005 and his B.Sc. from Sharif University of Technology in 1999. He joined Google Research in New York in 2008, after spending a couple of years at Microsoft Research, MIT and Amazon.com. He is the co-winner of a SODA05 best student paper award and ACM EC08 best paper award. His research areas include algorithms, algorithmic game theory, combinatorial optimization, and social networks analysis. At Google, he is mainly working on algorithmic and economic problems related to search and online advertising. Recently he is working on online ad allocation problems, distributed algorithms for large-scale graph mining, and mechanism design for advertising exchanges.
</p>

<li>
<img alt=""  src="pics/mpc/ben.jpg">
<h4>Benjamin Moseley (Washington University, St. Louis)</h4> 
<p align="left">
Benjamin Moseley joined the Department of Computer Science &amp; Engineering at Washington University in St. Louis in July 2014. Previously, Professor Moseley was a Research Assistant Professor at the Toyota Technological Institute at Chicago from 2012-2014. Prior to that, he was a visiting professor at Sandia National Laboratories in 2013 and a research intern at Yahoo! Research in 2010 and 2011.
</p>

<p align="left">
Professor Moseley received the Best Paper Award at the 2015 International Parallel and Distributed Processing Symposium (IPDPS), Best Paper Award at the 2013 Symposium on Parallelism in Algorithms and Architectures (SPAA) and Best Student Paper Award at the 2010 Symposium on Discrete Algorithms (SODA).
<p>

<p align="left">
Professor Moseley's research interests are broadly focused in the field of theoretical computer science. Specifically, he is interested in the design, analysis and limitations of algorithms. He is also interested in the applications of algorithms. Recently, his work has focused on problems arising in resource allocation, large data analysis and sustainable computing.
</p>

<li>
<img alt=""  src="pics/mpc/krzysztof.jpg">
<h4>Krzysztof Onak (IBM T.J. Watson Research Center)</h4> 
<p align="left">
Krzysztof Onak is a computer scientist who works at the IBM T.J. Watson Research Center near Yorktown Heights, NY. He is interested in computation with limited resources, including sublinear-time algorithms, streaming algorithms, and algorithms for modern parallel systems. Krzysztof received his Master's degree from the University of Warsaw and his PhD from the Massachusetts Institute of Technology. Before joining IBM, he was a Simons Postdoctoral Fellow at Carnegie Mellon University.
</p>
</ul>



<a id="orgs"><h1>Organizers and Support</h1></a>
<div>
<h2>Organizers:</h2>
<ul class="list-thumbnail">

<li>
<img alt=""  src="pics/mpc/ashish.jpg">
<h4>Ashish Goel (Stanford University)</h4> 
<p align="left">

Ashish Goel is a Professor of Management Science and Engineering and (by courtesy) Computer Science at Stanford University, and a member of Stanford's Institute for Computational and Mathematical Engineering. He received his PhD in Computer Science from Stanford in 1999, and was an Assistant Professor of Computer Science at the University of Southern California from 1999 to 2002. His research interests lie in the design, analysis, and applications of algorithms; current application areas of interest include social networks, participatory democracy, Internet commerce, and large scale data processing. Professor Goel is a recipient of an Alfred P. Sloan faculty fellowship (2004-06), a Terman faculty fellowship from Stanford, an NSF Career Award (2002-07), and a Rajeev Motwani mentorship award (2010). He was a co-author on the paper that won the best paper award at WWW 2009, and an Edelman Laureate in 2014.
</p>
<p align="left">
Professor Goel was a research fellow and technical advisor at Twitter, Inc. from July 2009 to Aug 2014.
</p>

<li>
<img alt=""  src="pics/mpc/sergei.jpg">
<h4>Sergei Vassilvitskii (Google Research, NYC)</h4> 
<p align="left">
Sergei Vassilvitskii is a Research Scientist at Google New York. Previously he was a Research Scientist at Yahoo! Research and an Adjunct Assistant Professor at Columbia University. He completed my PhD at Stanford Universty under the supervision of Rajeev Motwani. Prior to that he was an undergraduate at Cornell University.

</p>

<li>
<img alt=""  src="pics/mpc/grigory.jpg">
<h4>Grigory Yaroslavtsev (University of Pennsylvania)</h4> 
<p align="left">
Grigory Yaroslavtsev is a postdoctoral fellow at the Warren Center for Network and Data Sciences.He was previously a Postdoctoral Fellow in Mathematics at Brown University, ICERM. He received his Ph.D. in Theoretical Computer Science in 2013 from Pennsylvania State University and an M.Sc. in Applied Mathematics and Physics from the Academic University of the Russian Academy of Sciences in 2010. 
</p>
<p align="left">
Grigory works on efficient algorithms for sparsification, summarization and testing properties of large data, including approximation, parallel and online algorithms, learning theory and property testing, communication and information complexity and private data release.
</p>
</ul>


</body></html>


