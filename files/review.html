<html>
<head>
</head>
<body>
<h1>Review of an Applied Paper</h1>
<p>
This review is an obfuscated version of a real review I wrote. The name of the conference, all algorithms, problems, models and technical details involved are changed. Only general logical structure and basic mathematical observations are original. I never knew the names of the authors because of the double-blind review process. In its current form I believe that this review is anonymized so that it is generic enough to be published online. If you suspect that these anonymization measures are not sufficient please let me know and I will act promptly.
<p>
This paper, submitted to the ACM Conference on Big Data Processing, studies the problem of low-rank model fitting via agent-based multiplicative weights update method. This problem has attracted a lot of attention recently due to strong interest from practitioners of large data processing. The paper is very well written and explains technical details with clarity. I briefly recap the structure of the paper below.
<p>
The authors start by discussing an assumption A about the data that they intend to make. They give an experimental evaluation of this assumption, which has nothing to do with A. Instead, they test multiple other assumptions A1, A2, A3,..., A7 and conclude that they hold based on an empirical evidence. However, none of the Aiâ€™s or any combination of them implies A. The authors conclude that A holds, even though A is not formally defined and very vague. The authors use A to justify another assumption B, which has no relationship to A and implies that agent-based modeling can be used. Relying on B they formulate a low-rank model fitting problem C, which relies on agent-based information. In this formulation they assume independence of events, which are not independent. This allows to factor the problem nicely, but introduces a rescaling issue which is solved by some arbitrary choice of normalization, obtaining an optimization problem called C-normalized. Another arbitrary choice of normalization allows the authors to ensure that their low-rank model fitting optimization problem indeed may potentially have solutions of low rank. This gives an optimization problem C-doubly-normalized. The authors agree that what is low here is still unclear. However, there is only so much computational time they can use so the value of low is uniquely determined by the computational power of the system. In order to solve the low-rank model-fitting problem the authors employ a heuristic version of the randomized multiplicative weights update method, which comes with no guarantees. This method takes too much memory in practice, a barrier which is overcome by employing uniformly random sampling of constraints. The constraints are in fact sampled not uniformly, but indeed somewhat randomly. The authors give extensive experimental evaluation of their method on a variety of datasets. They compare performance of their algorithms against multiple other approaches, which solve the same vaguely defined problem of low-rank model fitting. They use some arbitrarily chosen set of metrics D, E, F to measure the performance of their algorithms, some taken from the previous work and some new. D, E and F have nothing to do with the objective of C-doubly-normalized, C-normalized or C. The authors conclude that performance of their algorithms according to D, E and F beats previous work by a mile. On the instances considered it is not clear whether metrics D, E or F favor a solution, which corresponds to low-rank model fitting.
</body>
</html>
